The following is an unstructured video transcript by David Shapiro. Write detailed notes from the transcript, but focus on this topic: AI Ethics

TRANSCRIPT:

go next yeah we're just going to make a small point about something somebody raised one or two people ago which was about um how google provides multiple results um and attempts to kind of distract you with a bun you know a bunch of ads and that that is kind of um fundamental to the way that uh i guess transactional commerce works without conversation with ai but one of the things that i've noticed working with this working working within a conversational ai gpt3 space as i have for the last few months is that conversational um recommendations are fundamentally like singular like it's or if they're not singular it's definitely not a long list of stuff so if you think about like how does google work um well you type type in a query and google gives you a list of stuff a list of a bunch of pages to go hunt and peck through or gives you a list of ads to go look through or a list of places to go look at but if you're in a conversation with your your friend and you're like hey where should we go for dinner tonight maybe they might list one maybe two places but it's fundamentally just more um just the the medium of conversation itself lends itself to singular or low number recommendations and actually think that has big implications for the way that uh commerce works or could work in the future like if you imagine a future that's more conversational uh in nature i i think that there's a responsibility on the ai to actually do more personalized recommendations to go the extra step and say okay i'm not going to be google and give you 20 possible options or 200 000 pages i'm going to condense all of that down to the one answer i think is right or i'm going to give you the you know recommendation on one product i think is right for you um so anyway a small point about just the the medium itself of conversational um ai i think changes the game uh on a lot of that yeah and it'll take a while to explore the space and figure out because that you know what you're talking about um from a from a cognitive perspective is in part cognitive control because like if you if you watch the original episodes of star trek the next generation and they ask data a question then he just starts rambling that's like gpg3 right now like he'll just ramble all the answers and it's like data shut up um but how do you know as a human like oh andy you asked me a question and how much filtration does my brain go through automatically to just give you the one answer that i know you need right so that is that is a huge component of what what we're going to need to do is because we do so much pre-selection of what it is that we're going to say and you know you look at people with adhd or um or people on the spectrum or if you're tired or hungry right your executive function is impaired and you don't know when to shut up and that's me sometimes that's why i talk so much sometimes um but yeah okay so there's a few other folks that wanted to jump in richard and jordan i think y'all were up next if you wanted to jump jump in yeah um so i think uh somebody brought up the the question about like liability um and i'm wondering like uh as this kind of technology gets more advanced uh what are your thoughts on like the whole um who's liable you know if your robot is running to the store and knocks down a grand mall like who's liable um and and i think that beg is a question of like for these ai assistants what does like um ownership look like just curious to hear what y'all think yeah great question christopher you want to jump in melody that is a topic everyone is debating right now even open ai who owns their model is currently debating ownership over let's just talk about if you have it write something is that because you had it you wrote a thesis then it unpacked the thesis so technically you're the writer and it's just an editor ownership is very difficult in this and just working into then liability like you're talking about is just opening up a huge can of worms a huge can of worms on um as far as that goes that's still being explored um we are not currently dealing with robotics but as that comes that will definitely uh open up greater questions of um yeah we don't know and nobody knows yet yeah i can jump in a little bit yeah i was gonna say um be because i've worked on fine-tuning chat bots and also the the um with cognitive control um when i first did my work on natural language cognitive architecture i came up with this idea of self-censorship and so this is this is based on neuroscience of the prefrontal cortex um the prefrontal cortex is the part of your brain that's just behind your forehead it's why humans have bigger heads than other primates and this is where a lot of the functionality in our brain happens that allows us to behave in polite society which a big chunk of that is self-censorship which is saying what you know i should not do this i'm going to control myself you know like you see a delicious piece of food you don't just grab it and stuff it in your face like you might expect a chimpanzee or a monkey to do right you know because lower lower animals are less intelligent animals with with less sophisticated social brains you know the more dominant animal is just going to grab the delicious piece of food smack the other one in the face and run off we don't do that as humans a big reason that we can control our behavior that way is is because of self-censorship and so one of the things that i explored with natural language cognitive architecture was one a self-censorship component which basically says it's basically evaluating its behavior and its decisions and it's saying okay i shouldn't do this so i'm just going to stop myself there and another thing is having a what i call the constitution which i've kind of moved away from this idea and towards the core objective functions or the heuristic imperatives but basically it's it's a document or a configuration file almost it says these are my values these are my


DETAILED NOTES ABOUT AI Ethics:

==========

-A big question in AI ethics is who owns the AI? -There is debate over who is liable if a robot causes damage. -One suggestion is that AI should have a "self-censorship" component to prevent it from doing things it shouldn't. -Another suggestion is that AI should have a "constitution" that outlines its values and objectives.