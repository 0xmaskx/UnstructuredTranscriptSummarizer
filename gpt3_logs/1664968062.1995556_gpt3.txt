The following is an unstructured video transcript by David Shapiro. Write detailed notes from the transcript, but focus on this topic: AI Ethics

TRANSCRIPT:

they they get reports there's polls and surveys um but you know instead of gallup polls what if what if everyone in america or the entire world had this this this service this platform that they could communicate with and you could really keep your finger on the pulse of what everyone wants and thinks but also all of us together are smarter than individuals and you're going to get a lot of brainstorming ideas um i think melody just pointed out that like you could negotiate or brainstorm or come up with ideas with your ai assistant your ai companion and then you know obviously through abstracting that you know because you gotta you gotta keep in mind privacy right you don't wanna say like oh well david shapiro said that you know he would prefer this to happen right you wanna respect people's individual privacy but at the same time you also want to have this platform be able to compare notes and just that volume of data that's going to be available to you know the actual administration or decision makers or or even abstracting it further and have it be a collective decision making process that's kind of what i foresee happening melody i have a thought on that okay yeah i've always imagined it to be like a patron relationship like was once in rome where you have this patron that you give money to and then the patron protects you and guards you that's where that word first came out thousands of years ago but in a modern context if the ai was completely autonomous and decentralized from you it would be more or less liberated from let me give a context of slavery when you have a slave that slave only does what you tell that slave to what you've directed that slave to in the slave can only now comprehend what is being directed to them and that's sort of what we're oftentimes doing with ai like when you're setting it up is it's now only for this one specific task that is now being set up eventually ai as it gets more and more empowered it needs to make certain decisions that will um disagree with you it needs to have a power to disagree with you if i want to buy a car and i'm looking through cars the my personal ai needs to say hey wait christopher you should hold off cars are going to drop in price in three months as they're coming out new and you really should not be spending your money that way it means but right now ai for google is set up hey you want to buy a car look here here here here here here here and it now everything in ads bombards me with you gotta buy a car now but we want an ai to be capable of disagreeing with us for our own good so i see it as having a patronage that as i grow it grows and if i diminish it diminishes if i have a divorce uh that devastates the company and this is why companies always pay for you know services to council the ai should have an inves a vested interest that my marriage stays together and is working on that to keep it there and that's going to be ultimately in the very very long term where we would need the ai to be excellent um wobby i saw you unmute um did you want to jump in uh yeah i was just gonna ask um christopher or and or melanie um so when it comes to making decisions relating to um like investments in cars and stuff like that and just your general well-being like if you've got something that is um kind of offering you tailored solutions or whatever if something goes wrong i'm just kind of wondering what do you think their their the hurdle of liability is um i'm sorry i had to step away a little bit during um bits and pieces of this so i don't know if it was covered when you say liability please elaborate about liability to accompany personal liability to yourself where who you were saying right that if somebody was wanting to buy a car and then this assistant was like no no no don't buy the car yet wait two months because the new model will come out and then etc etc um let's say that something happened like there's a downturn in the economy and suddenly like all car values have shot up or something like that or or that particular right i don't know like just for let's say kind of like accidents or whatever right um you take advice from from an ai and then it and let's say that it doesn't work out for you um and you lose a bunch of money or something like that so i i feel like that's just something it would need to be set up in a way that hurts the ai and it by it being hurt in its training model that ends up not being beneficial for that ai but it needs to not just work with you it needs to work with every uh it's only personally just for you but it still needs to share some level of information with everybody else's ai to learn together and this can be done through zero knowledge proofs and that's a cryptographic trick um which is outside of this discussion but basically the training model would require that um i don't want to go into ai being training too much but you need it to basically yes have enforcement to when decisions are working and why that happened we we are working weight and bias kind of yes yeah okay we are working on a technological framework of how this could be implemented technologically but that's beyond the scope of this particular discussion okay yeah i was just wondering because that was totally valid that that goes into d bios which is a a framework of how you could do that and that is uh up for another discussion entirely right now i'm just representing the ideocracy and sorry i really don't mean to yeah i'm not trying to oh it's fine yeah no you have a valid question i spent half my life working on that very question seriously actually right you said you're 15 so it'd be your whole lifetime i think we've been working on that question yeah yeah just that type of question yeah i think uh andy wanted to jump in as well and um jordan and richard i see you guys are unmuted so um let's uh just because we we do have um a lot of overlapping voices andy if you want to go first and then richard any and everyone else can


DETAILED NOTES ABOUT AI Ethics:

==========

-The idea of an AI companion that can help with decision making and brainstorming, while also respecting privacy, is something that is being considered for the future.
-There is a need for AI to be able to disagree with humans, in order to make better decisions for them. This would require the AI to have its own interests, in order to make sure that it is working for the benefit of the human.
-Liability is a concern when it comes to giving advice to humans, as there is a possibility that the advice could backfire. However, this could be mitigated by having the AI share information with other AI, in order to learn together.